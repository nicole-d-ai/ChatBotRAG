{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicole-d-ai/ChatBotRAG/blob/main/TP5_chatbot_con_RAG_Nicole_Ferreyra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motor de b√∫squeda"
      ],
      "metadata": {
        "id": "s3bs1FmWkfrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* B√∫squeda por palabras clave: Extrae palabras clave de la pregunta del usuario y busca coincidencias en las preguntas almacenadas.\n",
        "\n",
        "* Similitud del coseno: Si has representado las preguntas como vectores (por ejemplo, usando TF-IDF o word embeddings), puedes usar la similitud del coseno para medir la distancia entre las preguntas.\n",
        "\n",
        "* Word embeddings: Utiliza modelos de word embeddings como Word2Vec o BERT para obtener representaciones sem√°nticas de las preguntas y las consultas del usuario."
      ],
      "metadata": {
        "id": "enbz9kXGkWlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B√∫squeda por palabras claves"
      ],
      "metadata": {
        "id": "UQoVRp4gjxUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tu_diccionario = {\n",
        "   \"hola\": \"¬°Hola! ¬øEn qu√© puedo ayudarte?\",\n",
        "   \"adi√≥s\": \"Hasta luego. ¬°Que tengas un buen d√≠a!\",\n",
        "   \"informaci√≥n\": \"¬øQu√© tipo de informaci√≥n est√°s buscando?\",\n",
        "   # Agrega m√°s entradas de diccionario seg√∫n tus necesidades\n",
        "}\n"
      ],
      "metadata": {
        "id": "3BljEMEOhpTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_pregunta(pregunta):\n",
        "    pregunta_procesada = nlp(pregunta.lower())  # Procesa la pregunta y convierte a min√∫sculas\n",
        "    respuesta = \"Lo siento, no entiendo tu pregunta.\"\n",
        "\n",
        "    # Busca una coincidencia en el diccionario\n",
        "    for palabra in pregunta_procesada:\n",
        "        # regresa la primer coincidencia que encuentra\n",
        "        if palabra.text in tu_diccionario:\n",
        "            respuesta = tu_diccionario[palabra.text]\n",
        "            break\n",
        "\n",
        "    return respuesta\n"
      ],
      "metadata": {
        "id": "Z4q_k1_3hvyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#while True:\n",
        "#    entrada_usuario = input(\"T√∫: \")\n",
        "#    if entrada_usuario.lower() == \"salir\":\n",
        "#        print(\"Chatbot: Hasta luego.\")\n",
        "#        break\n",
        "#    respuesta = responder_pregunta(entrada_usuario)\n",
        "#    print(\"Chatbot:\", respuesta)\n"
      ],
      "metadata": {
        "id": "9gMPKi-ghwcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede modificar la implementaci√≥n anterior para evitar el bucle while True y usar un dataset de prueba de preguntas y respuestas."
      ],
      "metadata": {
        "id": "6S88twrY5sOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B√∫squeda por similitud"
      ],
      "metadata": {
        "id": "nI2FsyUOj3je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para los chatbots basados ‚Äã‚Äãen recuperaci√≥n, es com√∫n utilizar bolsas de palabras (bag of words) o tf-idf para calcular la similitud de intenciones."
      ],
      "metadata": {
        "id": "AoZIaX0Kj7xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Datos de ejemplo\n",
        "preguntas = [\"¬øQu√© es el aprendizaje autom√°tico?\",\n",
        "             \"¬øC√≥mo funciona la regresi√≥n lineal?\"]\n",
        "respuestas = [\"El aprendizaje autom√°tico es una rama de la inteligencia artificial...\",\n",
        "              \"La regresi√≥n lineal es un m√©todo de modelado...\"]\n",
        "\n",
        "# Vectorizaci√≥n con TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preguntas)\n",
        "\n",
        "# Funci√≥n para encontrar la mejor coincidencia\n",
        "def responder_pregunta(consulta_usuario):\n",
        "    consulta_vec = vectorizer.transform([consulta_usuario])\n",
        "    similitudes = cosine_similarity(consulta_vec, tfidf_matrix).flatten()\n",
        "    print(similitudes)\n",
        "    indice_mejor_coincidencia = similitudes.argmax()\n",
        "    print(indice_mejor_coincidencia)\n",
        "    return respuestas[indice_mejor_coincidencia]\n"
      ],
      "metadata": {
        "id": "CReIz0ISj75s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ejemplo de consulta\n",
        "consulta = \"¬øQu√© es la regresi√≥n lineal?\"\n",
        "print(responder_pregunta(consulta))\n"
      ],
      "metadata": {
        "id": "foqaZ1FN583i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B√∫squeda por similitud en embeddings"
      ],
      "metadata": {
        "id": "20KCxfCymOHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes vectorizar el texto usando embeddings, de spacy por ejemplo, como vimos en clases.\n"
      ],
      "metadata": {
        "id": "7g6jgLSLnilX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hghx5BwKdm-A"
      },
      "outputs": [],
      "source": [
        "#!pip install spacy --quiet\n",
        "#!python -m spacy download es_core_news_sm --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gfClChCB9VgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actividades"
      ],
      "metadata": {
        "id": "DEmDe4Xy8OVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1) Elaborar un dataset de preguntas y respuestas para crear un Chatbot para un aplicaci√≥n particular. ( 3 puntos )\n",
        "\n",
        "1.1 Debe definir la aplicaci√≥n (atenci√≥n al cliente bancario, atenci√≥n a estudiantes universitarios, etc).\n",
        "\n",
        "1.2 El listado de preguntas y respuestas debe tener como m√≠nimo 20 elementos pregunta - respuesta.\n",
        "\n",
        "###  2) Crear el chatbot utilizando TFIDF y similitud del coseno. (1 punto)\n",
        "\n",
        "### 3) Crear otro chatbot utilizando embeddings. Indique cu√°l embedding (1 punto) pre-entrenado eligi√≥.\n",
        "\n",
        "### 4) Muestra ambos chatbots funcionando (1 punto)\n",
        "\n",
        "Adjuntar la lista de preguntas y respuestas utilizadas para probar el funcionamiento.\n",
        "\n",
        "Releve o indique cu√°les respondi√≥ correctamente y cu√°les no.\n",
        "\n",
        "### 5) A√±ade tus conclusiones de todo lo realizado (2 punto)\n",
        "\n",
        "* Resalte e indique en cu√°les respuestas falla o tiene problemas.\n",
        "* Cu√°les preguntas confunde.\n",
        "* Compare los resultados de los chatbots.\n",
        "\n",
        "\n",
        "\n",
        "### No olvides:\n",
        "\n",
        "* Explicar tus decisiones y configuraciones. A√±adir tus conclusiones.\n",
        "* Anunciar en el foro cu√°l ser√° tu aplicaci√≥n y postear tu entrega y tus avances.\n",
        "* Debes subir tu notebook a un repo GitHub p√∫blico de tu propiedad compartido + enlace colab.\n",
        "* Documentar todo el proceso.\n",
        "* Citar tus fuentes\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1sGxF1VglJVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8HHwzwm19vAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "61v1dfVT9jq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEADLINE ENTREGA HASTA PUNTO 5 PARA REGULARIZAR O PROMOCI√ìN** DOMINGO 09/11"
      ],
      "metadata": {
        "id": "gSrxZ1568m5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset de preguntas y respuestas ‚ùì\n",
        "\n",
        "-La app se basa en ofrecer \"Servicios de oficios\" consta de dos tipos de usuarios, cliente y prestador de servicios. como por ejemplo (plomeros, electricistas, alba√±iles, carpinteros, etc)"
      ],
      "metadata": {
        "id": "MfxBoqIuBKO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© Justificaci√≥n del campo `rol`\n",
        "\n",
        "El campo **rol** se incluy√≥ en el dataset para diferenciar el tipo de usuario al que va dirigida cada pregunta:\n",
        "- **Cliente:** quien busca contratar un servicio.\n",
        "- **Profesional:** quien ofrece su servicio.\n",
        "- **General:** consultas que pueden hacer ambos.\n",
        "\n",
        "Aunque en este TP no se utiliz√≥ directamente para filtrar respuestas, se pens√≥ para una futura expansi√≥n del chatbot, donde las respuestas se adapten seg√∫n el tipo de usuario.  \n",
        "Por ejemplo, si el usuario se identifica como *profesional*, el chatbot solo buscar√≠a coincidencias dentro de las preguntas etiquetadas con ese rol, mejorando la precisi√≥n y personalizaci√≥n de las respuestas."
      ],
      "metadata": {
        "id": "KlxR_sEnu6zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = [\n",
        "    {\"rol\":\"general\",\"pregunta\":\"¬øQu√© tipo de servicio busc√°s?\",\n",
        "     \"respuesta\":\"Ofrecemos: Plomer√≠a, Electricidad, Gasista, Alba√±iler√≠a, Pintura, Carpinter√≠a, Cerrajer√≠a, Jardiner√≠a, Limpieza y Dise√±o de interiores. Decime tu zona.\"},\n",
        "\n",
        "    {\"rol\":\"general\",\"pregunta\":\"¬øC√≥mo funciona la app?\",\n",
        "     \"respuesta\":\"Eleg√≠s categor√≠a y ubicaci√≥n; ves profesionales cercanos con precio, rese√±as y certificaciones. Pod√©s chatear, reservar y pagar seguro.\"},\n",
        "\n",
        "    {\"rol\":\"cliente\",\"pregunta\":\"Necesito un electricista urgente\",\n",
        "     \"respuesta\":\"¬øQu√© ocurri√≥ (corte, disyuntor, enchufe quemado, chisporroteo) y en qu√© barrio est√°s? Te muestro opciones con disponibilidad hoy.\"},\n",
        "\n",
        "    {\"rol\":\"cliente\",\"pregunta\":\"Busco gasista matriculado\",\n",
        "     \"respuesta\":\"Tenemos gasistas con matr√≠cula. ¬øInstalaci√≥n, mantenimiento o reparaci√≥n? Indic√° tu zona para cotizaciones.\"},\n",
        "\n",
        "    {\"rol\":\"cliente\",\"pregunta\":\"¬øC√≥mo veo si tiene certificaci√≥n?\",\n",
        "     \"respuesta\":\"Entrando al perfil: ver√°s matr√≠cula/certificados, fotos de trabajos, ratings y precio por hora/proyecto.\"},\n",
        "\n",
        "    {\"rol\":\"cliente\",\"pregunta\":\"¬øPuedo pedir presupuesto sin cargo?\",\n",
        "     \"respuesta\":\"S√≠, pod√©s enviar tu pedido a varios profesionales y recibir presupuestos estimados sin costo.\"},\n",
        "\n",
        "    {\"rol\":\"profesional\",\"pregunta\":\"Quiero ofrecer mis servicios\",\n",
        "     \"respuesta\":\"Cre√° tu perfil: foto, rubros, zona, disponibilidad, precio y matr√≠cula/certificados si aplica. Luego verificamos tu identidad.\"},\n",
        "\n",
        "    {\"rol\":\"profesional\",\"pregunta\":\"¬øC√≥mo fijo mi precio?\",\n",
        "     \"respuesta\":\"Eleg√≠ precio por hora o por proyecto. Sugerimos valores seg√∫n rubro y zona basados en el mercado local.\"},\n",
        "\n",
        "    {\"rol\":\"general\",\"pregunta\":\"¬øC√≥mo cancelo una reserva?\",\n",
        "     \"respuesta\":\"Desde Mis Reservas > Detalle > Cancelar. Si faltan menos de 12 h puede aplicarse un cargo seg√∫n pol√≠tica del profesional.\"},\n",
        "\n",
        "    {\"rol\":\"general\",\"pregunta\":\"¬øEs seguro contratar por ac√°?\",\n",
        "     \"respuesta\":\"Verificamos identidad y certificaciones. Los pagos est√°n protegidos y hay seguro de responsabilidad seg√∫n el rubro.\"},\n",
        "\n",
        "    {\"rol\": \"cliente\", \"pregunta\": \"¬øC√≥mo pago el servicio?\",\n",
        "     \"respuesta\": \"Pod√©s pagar con tarjeta, transferencia o efectivo al finalizar. Si contrat√°s desde la app, el pago queda protegido hasta que confirmes el trabajo.\"},\n",
        "\n",
        "    {\"rol\":\"cliente\",\"pregunta\":\"¬øQu√© pasa si el profesional no viene?\",\n",
        "     \"respuesta\":\"Pod√©s cancelar sin costo y dejar una rese√±a. Nuestro equipo de soporte puede ayudarte a reprogramar o asignarte otro profesional.\"},\n",
        "\n",
        "    {\"rol\":\"cliente\",\"pregunta\":\"¬øPuedo hablar con el profesional antes de contratar?\",\n",
        "     \"respuesta\":\"S√≠, pod√©s usar el chat interno para consultar dudas, precios y disponibilidad antes de confirmar la reserva.\"},\n",
        "\n",
        "    {\"rol\":\"cliente\",\"pregunta\":\"¬øQu√© garant√≠a tengo si el trabajo sale mal?\",\n",
        "     \"respuesta\":\"Ten√©s 7 d√≠as de garant√≠a por mano de obra. Si algo no qued√≥ bien, pod√©s abrir un reclamo desde la app y lo resolvemos contigo.\"},\n",
        "\n",
        "    {\"rol\":\"profesional\",\"pregunta\":\"¬øC√≥mo mejoro mi perfil?\",\n",
        "     \"respuesta\":\"Complet√° todos tus datos, agreg√° fotos de tus trabajos, respond√© r√°pido los mensajes y manten√© buenas calificaciones para subir en el ranking.\"},\n",
        "\n",
        "    {\"rol\":\"profesional\",\"pregunta\":\"¬øC√≥mo recibo el pago?\",\n",
        "     \"respuesta\":\"Si es una reserva online, el dinero se acredita en tu billetera virtual y pod√©s transferirlo a tu cuenta bancaria.\"},\n",
        "\n",
        "    {\"rol\":\"profesional\",\"pregunta\":\"¬øPuedo pausar mis servicios?\",\n",
        "     \"respuesta\":\"S√≠, desde Configuraci√≥n pod√©s poner tu perfil en pausa temporal sin perder tus datos ni calificaciones.\"},\n",
        "\n",
        "    {\"rol\":\"profesional\",\"pregunta\":\"¬øC√≥mo verifican mi identidad?\",\n",
        "     \"respuesta\":\"Te pedimos foto del DNI y una selfie. Tambi√©n pod√©s subir tu matr√≠cula o certificaciones si tu rubro lo requiere.\"},\n",
        "\n",
        "    {\"rol\":\"general\",\"pregunta\":\"¬øC√≥mo contacto al soporte?\",\n",
        "     \"respuesta\":\"Pod√©s escribirnos desde la secci√≥n 'Ayuda' en la app o por email a soporte@miapp.com. Atendemos todos los d√≠as de 9 a 21 hs.\"},\n",
        "\n",
        "    {\"rol\":\"general\",\"pregunta\":\"¬øPuedo dejar una rese√±a del profesional?\",\n",
        "     \"respuesta\":\"S√≠, despu√©s de que el trabajo finaliza, pod√©s calificar con estrellas y dejar un comentario sobre tu experiencia.\"},\n",
        "]\n"
      ],
      "metadata": {
        "id": "1AgF3NB_A9LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ü§ñ Crear el chatbot utilizando TF-IDF y similitud del coseno\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lv_mIC1JEYrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 0: importar librer√≠as\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Paso 1: listas desde el diccionario 'qa'\n",
        "preguntas  = [x[\"pregunta\"]  for x in qa]\n",
        "respuestas = [x[\"respuesta\"] for x in qa]\n",
        "\n",
        "# Paso 2: modelo TF-IDF (representaci√≥n de texto)\n",
        "tfidf = TfidfVectorizer(lowercase=True, ngram_range=(1,2))\n",
        "X = tfidf.fit_transform(preguntas)\n",
        "\n",
        "# Paso 3: funci√≥n del chatbot TF-IDF\n",
        "def responder_tfidf(consulta):\n",
        "    vector = tfidf.transform([consulta])             # Convertimos consulta a n√∫meros\n",
        "    sims   = cosine_similarity(vector, X).ravel()    # Comparaci√≥n con TODAS las preguntas\n",
        "    idx    = int(np.argmax(sims))                    # √çndice de la mejor coincidencia\n",
        "\n",
        "    return {\n",
        "        \"pregunta\": preguntas[idx],\n",
        "        \"respuesta\": respuestas[idx],\n",
        "        \"score\": float(sims[idx])\n",
        "    }\n",
        "\n",
        "# Prueba r√°pida\n",
        "responder_tfidf(\"Busco un gasista con matr√≠cula\")\n"
      ],
      "metadata": {
        "id": "QBakWkHyEg_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Chatbot con Embeddings con spaCy (pre-entrenado en espa√±ol)"
      ],
      "metadata": {
        "id": "yITt8fFTwq92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"numpy<2.0\" spacy==3.7.4\n",
        "!python -m spacy download es_core_news_md --direct -q"
      ],
      "metadata": {
        "id": "Xl05EVQ3okws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalamos y cargamos el embedding\n",
        "#!pip -q install \"spacy==3.7.4\"\n",
        "#!python -m spacy download es_core_news_md --direct -q\n"
      ],
      "metadata": {
        "id": "dX8bj7Pa6mBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Paso 1: importar y cargar el modelo\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_md\")"
      ],
      "metadata": {
        "id": "1QW0pl6CxIEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertir las preguntas en vectores\n",
        "Q = np.vstack([nlp(q).vector for q in preguntas])\n",
        "Q.shape"
      ],
      "metadata": {
        "id": "agniWwZ-77Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funci√≥n para responder con embeddings"
      ],
      "metadata": {
        "id": "me9EWh5npFov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_embed(consulta, top_k=1, devolver_scores=False):\n",
        "    v = nlp(consulta).vector.reshape(1, -1)       # vector de la consulta\n",
        "    sims = cosine_similarity(v, Q).ravel()        # similitud con todas las preguntas\n",
        "    idxs = sims.argsort()[::-1][:top_k]           # top-k m√°s parecidas\n",
        "\n",
        "    resultados = []\n",
        "    for i in idxs:\n",
        "        resultados.append({\n",
        "            \"pregunta_match\": preguntas[i],\n",
        "            \"respuesta\": respuestas[i],\n",
        "            \"score\": float(sims[i])\n",
        "        })\n",
        "    return resultados if devolver_scores else resultados[0]\n",
        "\n",
        "#Probamos con una consulta\n",
        "responder_embed(\"certificado\")"
      ],
      "metadata": {
        "id": "6qbHOTYIpGbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset de prueba (preguntas nuevas)\n",
        "preguntas_prueba = [\n",
        "    \"¬øC√≥mo puedo registrarme como profesional?\",\n",
        "    \"¬øPuedo cancelar una reserva sin pagar multa?\",\n",
        "    \"¬øD√≥nde veo las rese√±as de un trabajador?\",\n",
        "    \"¬øQu√© formas de pago tienen disponibles?\",\n",
        "    \"¬øC√≥mo se calcula el precio del servicio?\",\n",
        "    \"¬øQu√© pasa si el profesional no se presenta?\",\n",
        "    \"¬øC√≥mo modifico mi zona de trabajo?\",\n",
        "    \"¬øEs seguro contratar por la app?\",\n",
        "    \"¬øPuedo hablar con el profesional antes de contratar?\",\n",
        "    \"¬øC√≥mo califico un servicio?\"\n",
        "]"
      ],
      "metadata": {
        "id": "bSzYkKmixvy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados = []\n",
        "\n",
        "for p in preguntas_prueba:\n",
        "    r_tfidf = responder_tfidf(p)\n",
        "    r_embed = responder_embed(p)\n",
        "\n",
        "    resultados.append({\n",
        "        \"Pregunta de prueba\": p,\n",
        "        \"Respuesta TF-IDF\": r_tfidf[\"respuesta\"],\n",
        "        \"Score TF-IDF\": round(r_tfidf[\"score\"], 3),\n",
        "        \"Respuesta Embeddings\": r_embed[\"respuesta\"],\n",
        "        \"Score Embeddings\": round(r_embed[\"score\"], 3)\n",
        "    })"
      ],
      "metadata": {
        "id": "LP5SZ1nb1dCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "df_resultados\n"
      ],
      "metadata": {
        "id": "fko_MzX0xmrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß≠ Conclusiones\n",
        "\n",
        "Durante las pruebas, ambos chatbots respondieron correctamente la mayor√≠a de las preguntas, pero con diferencias claras:\n",
        "\n",
        "- **TF-IDF:** respondi√≥ bien cuando la consulta del usuario ten√≠a palabras id√©nticas a las del dataset.  \n",
        "  Fall√≥ en frases m√°s cortas o con sin√≥nimos (ej: ‚Äúgasista certificado‚Äù no coincid√≠a con ‚Äúgasista con matr√≠cula‚Äù).  \n",
        "  Los scores fueron m√°s variables, entre 0.3 y 0.9.\n",
        "\n",
        "- **Embeddings (spaCy):** entendi√≥ mejor el significado general, incluso cuando las palabras no coincid√≠an exactamente.  \n",
        "  En general, tuvo **scores de similitud m√°s altos** y respuestas m√°s coherentes en consultas con sin√≥nimos o frases largas.  \n",
        "  Fall√≥ en casos donde dos preguntas ten√≠an sentidos parecidos (‚Äúcancelar‚Äù vs ‚Äúreprogramar‚Äù), lo que llev√≥ a confusi√≥n.\n",
        "\n",
        "**Comparaci√≥n general:**\n",
        "- TF-IDF ‚Üí m√°s literal, depende del texto exacto.  \n",
        "- Embeddings ‚Üí m√°s sem√°ntico, entiende mejor el sentido.\n",
        "\n",
        "**Conclusi√≥n final:**  \n",
        "El modelo basado en embeddings logra una interacci√≥n m√°s natural y flexible, mientras que el modelo TF-IDF es m√°s sencillo pero limitado a coincidencias exactas.  \n",
        "Ambos modelos son v√°lidos para diferentes objetivos: TF-IDF es ideal para un FAQ fijo y r√°pido, mientras que Embeddings es m√°s apropiado para chatbots que deben entender lenguaje variado.\n",
        "\n",
        "### üìö Referencias\n",
        "\n",
        "- Documentaci√≥n oficial de spaCy: https://spacy.io  \n",
        "- Scikit-Learn TF-IDF Vectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  \n",
        "- Curso Procesamiento del Habla ‚Äì Instituto Superior Santo Domingo (2025)  \n",
        "- Conversaci√≥n y gu√≠a de implementaci√≥n con ChatGPT (OpenAI, 2025)\n"
      ],
      "metadata": {
        "id": "lzL9sHjy1j6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP-5 ChatBot RAG"
      ],
      "metadata": {
        "id": "LT8PErzOHKC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset de prueba o evaluaci√≥n"
      ],
      "metadata": {
        "id": "vRqIumxaxqQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_soluciones_cliente = [\n",
        "    {\n",
        "        \"id\": 0,\n",
        "        \"pregunta\": \"¬øQu√© datos tengo que completar para crear mi cuenta en la app\",\n",
        "        \"respuesta\": \"Solo necesit√°s tu nombre, email o tel√©fono, una contrase√±a y tu ubicaci√≥n\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":1,\n",
        "        \"pregunta\": \"¬øPara que debo activar mi ubicaci√≥n?\",\n",
        "        \"respuesta\": \"Necesitamos tu ubicaci√≥n para encontrar tus profesionales mas cercanos.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":2,\n",
        "        \"pregunta\": \"¬øPuedo pedir m√°s de un presupuesto al mismo tiempo?\",\n",
        "        \"respuesta\": \"S√≠, pod√©s enviar tu solicitud a varios profesionales y comparar sus precios y tiempos de respuesta.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":3,\n",
        "        \"pregunta\": \"¬øLa app avisa cuando el profesional est√° llegando?\",\n",
        "        \"respuesta\": \"S√≠, te notificamos por la app cuando el profesional acepta, sale hacia tu domicilio y cuando est√° a pocos minutos.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":4,\n",
        "        \"pregunta\": \"¬øC√≥mo s√© si un profesional est√° disponible hoy?\",\n",
        "        \"respuesta\": \"En el perfil de cada profesional veras su disponibilidad diaria, incluso podes acordar horario y dia en el chat.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":5,\n",
        "        \"pregunta\" : \"¬øQu√© hago si tuve un problema con el servicio?\",\n",
        "        \"respuesta\": \"Pod√©s abrir un reclamo desde la secci√≥n de Ayuda. Nuestro equipo te acompa√±a para resolverlo con el profesional.\"\n",
        "    },\n",
        "   {\n",
        "        \"id\":6,\n",
        "        \"pregunta\": \"¬øC√≥mo elijo al profesional m√°s confiable?\",\n",
        "        \"respuesta\": \"Pod√©s guiarte por las calificaciones, la cantidad de trabajos realizados y los comentarios de otros usuarios.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":7,\n",
        "        \"pregunta\": \"¬øPuedo cambiar la fecha del servicio despu√©s de reservar?\",\n",
        "        \"respuesta\": \"S√≠, desde Mis Reservas pod√©s cambiar d√≠a y horario. El profesional debe aceptar la nueva fecha.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":8,\n",
        "        \"pregunta\": \"¬øQu√© pasa si el precio final no coincide con el presupuesto?\",\n",
        "        \"respuesta\": \"Si el precio difiere, pod√©s hablarlo por chat. Si no lleg√°s a un acuerdo, abr√≠ un reclamo desde Ayuda.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":9,\n",
        "        \"pregunta\": \"¬øPuedo guardar profesionales como favoritos?\",\n",
        "        \"respuesta\": \"S√≠, en cada perfil ten√©s la opci√≥n 'Guardar'. Podr√°s encontrarlos luego en tu lista de favoritos.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":10,\n",
        "        \"pregunta\": \"¬øC√≥mo veo trabajos anteriores del profesional?\",\n",
        "        \"respuesta\": \"En su perfil pod√©s ver fotos de trabajos, recomendaciones y su experiencia por rubro.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":11,\n",
        "        \"pregunta\": \"¬øQu√© significa cuando un profesional tiene 'Alta demanda'?\",\n",
        "        \"respuesta\": \"Quiere decir que tiene muchas reservas recientes. Puede tener menos disponibilidad, pero suele ser muy recomendado.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":12,\n",
        "        \"pregunta\": \"¬øC√≥mo env√≠o fotos del problema para pedir un presupuesto?\",\n",
        "        \"respuesta\": \"Pod√©s adjuntar fotos desde el chat o al crear la solicitud, as√≠ el profesional puede cotizar mejor.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":13,\n",
        "        \"pregunta\": \"¬øPuedo pedir un servicio para otra persona?\",\n",
        "        \"respuesta\": \"S√≠, solo asegurate de poner la direcci√≥n correcta y avisar al profesional por chat.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":14,\n",
        "        \"pregunta\": \"¬øC√≥mo s√© cu√°nto tardar√° en responder un profesional?\",\n",
        "        \"respuesta\": \"En el perfil ten√©s el tiempo promedio de respuesta basado en sus √∫ltimas conversaciones.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":15,\n",
        "        \"pregunta\": \"¬øQu√© hago si el presupuesto me parece muy alto?\",\n",
        "        \"respuesta\": \"Pod√©s pedir m√°s cotizaciones a otros profesionales y comparar opciones antes de decidir.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":16,\n",
        "        \"pregunta\": \"¬øPuedo modificar mi direcci√≥n despu√©s de crear la cuenta?\",\n",
        "        \"respuesta\": \"S√≠, desde Perfil > Direcci√≥n pod√©s actualizar tu domicilio cuando quieras.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":17,\n",
        "        \"pregunta\": \"¬øLa app me avisa cuando el profesional termina el trabajo?\",\n",
        "        \"respuesta\": \"S√≠, te llega una notificaci√≥n y pod√©s confirmar el servicio para habilitar el pago.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":18,\n",
        "        \"pregunta\": \"¬øC√≥mo veo si un profesional trabaja fines de semana?\",\n",
        "        \"respuesta\": \"En su perfil pod√©s ver su disponibilidad por d√≠as y horarios, incluyendo feriados o fines de semana.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":19,\n",
        "        \"pregunta\": \"¬øPuedo pedir un servicio recurrente, como limpieza semanal?\",\n",
        "        \"respuesta\": \"S√≠, pod√©s coordinar servicios recurrentes directamente con el profesional desde el chat.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\":20,\n",
        "        \"pregunta\": \"¬øQu√© hago si el profesional me quiere cobrar por fuera de la app?\",\n",
        "        \"respuesta\": \"Por seguridad no recomendamos pagos por fuera. Si pasa, avis√° en Ayuda para que podamos asistirte.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "XVTKoudRxp00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úî Modelo LLM de HuggingFace + Embeddings"
      ],
      "metadata": {
        "id": "GDgh3pmaCMTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ê LLM ELEGIDO: mistralai/Mistral-7B-Instruct-v0.2\n",
        "\n",
        "**Mistral-7B-Instruct y el tono ‚Äúb√°sico‚Äù**\n",
        "\n",
        "Lo elegi  porque:\n",
        "\n",
        "‚úî Entiende bien el espa√±ol informal\n",
        "\n",
        "‚úî Soporta preguntas cortas (\"como se hace para..?\", \"que es esto..?\" etc.)\n",
        "\n",
        "‚úî  Puede responder simple, sin que sea tecnico\n",
        "\n",
        "‚úî Compatible con HuggingFace y Transformers\n",
        "\n",
        "‚úî Ideal para sistemas RAG\n",
        "\n",
        "# ‚≠ê Embeddings elegidos\n",
        "\n",
        "**üîπ 1. sentence-transformers/all-MiniLM-L6-v2**\n",
        "\n",
        "* Es un de los modelos mas usados para b√∫squeda sem√°ntica, ideal para sistemas RAG\n",
        "\n",
        "* Funciona bien con preguntas informales\n",
        "\n",
        "* Es un modelo que maneja frases en espa√±ol aunque esten mal escritas o incomplentas\n",
        "\n",
        "**üîπ 2. jinaai/jina-embeddings-v2-base-es**\n",
        "\n",
        "* Interpreta mejor expresiones y variaciones informales por ejemplo (\"pa arreglar\", \"la canilla pierde\", \"viene hoy\")\n",
        "\n",
        "* Me permite evaluar si un embedding en espa√±ol recupera mejor el contexto, cumpliendo el objetivo comparativo\n",
        "\n",
        "* Robusto a preguntas cortas, ambiguas\n",
        "\n",
        "* Es f√°cil de interpretar con ChromaDB y sistemas RAG"
      ],
      "metadata": {
        "id": "2qOnuhJAGmGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì¶ 1) Instalar librer√≠as"
      ],
      "metadata": {
        "id": "cV89duORNLLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Librer√≠a para usar la base de datos vectorial\n",
        "#donde voy a guardar los embeddings de mis preguntas y respuestas\n",
        "\n",
        "!pip install chromadb --quiet"
      ],
      "metadata": {
        "id": "BunfJEfBCOXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instalaci√≥n que me permite cargar los modelos de embeddings\n",
        "\n",
        "!pip install sentence-transformers --quiet"
      ],
      "metadata": {
        "id": "NtYuP7hXPDpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalo transformers, libreria de HuggingFace para cargar el modelo LLM\n",
        "\n",
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "id": "m3N26VBEPSPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Me ayuda a que los modelos grandes se carguen bien en la GPU/CPU de colab y manejar los recursos sin errores\n",
        "\n",
        "!pip install accelerate --quiet"
      ],
      "metadata": {
        "id": "vkChnKCbPi3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero convierto mi base de conocimiento en vectores usando sentence-transformers.\n",
        "Esos vectores los guardo en una base de datos vectorial con chromadb.\n",
        "Cuando el usuario hace una pregunta, busco los vectores m√°s similares en Chroma y con ese contexto le paso todo al modelo de lenguaje Mistral-7B-Instruct, que cargo con transformers.\n",
        "accelerate me ayuda a que todo esto corra bien en la GPU de Colab.‚Äù"
      ],
      "metadata": {
        "id": "mgvmalNPP7mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß© MiniLM + Chroma"
      ],
      "metadata": {
        "id": "Hcl4KmcrByYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "#Cargamos el modelo de embeddings MiniLM\n",
        "embedding_model_minilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "#Creamos el cliente de ChromaDB\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "#Creamos la colleccion\n",
        "collection_minilm = chroma_client.create_collection(name=\"oficios_minilm\")\n",
        "\n",
        "#Documentos + ids + metadatos\n",
        "documents = []\n",
        "ids = []\n",
        "metadata = []\n",
        "\n",
        "for i, item in enumerate(qa):\n",
        "  texto = f\"Pregunta: {item['pregunta']}\\nRespuesta: {item['respuesta']}\"\n",
        "  documents.append(texto)\n",
        "  ids.append(f\"doc_{i}\")\n",
        "  metadata.append({\"rol\": item[\"rol\"]})\n",
        "\n",
        "#Calcular embeddings\n",
        "embedding_minilm = embedding_model_minilm.encode(documents).tolist()\n",
        "\n",
        "#Guardar la collecion en chromaDB\n",
        "collection_minilm.add(\n",
        "    documents=documents,\n",
        "    ids=ids,\n",
        "    metadatas=metadata,\n",
        "    embeddings=embedding_minilm\n",
        ")\n",
        "\n",
        "len(documents), \"documentos indexados en oficios_minilm\""
      ],
      "metadata": {
        "id": "79afSXuQBwHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar el modelo Jina-es"
      ],
      "metadata": {
        "id": "9QBDrEnEA700"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_jina = SentenceTransformer(\"jinaai/jina-embeddings-v2-base-es\")\n",
        "\n",
        "\n",
        "collection_jina = chroma_client.create_collection(name=\"oficios_jina\")\n",
        "\n",
        "# 3. Vectorizar (usando los mismos documents)\n",
        "embedding_jina = embedding_model_jina.encode(documents).tolist()\n",
        "\n",
        "# 4. Guardar en Chroma\n",
        "collection_jina.add(\n",
        "    documents=documents,\n",
        "    ids=ids,\n",
        "    metadatas=metadata,\n",
        "    embeddings=embedding_jina\n",
        ")\n",
        "\n",
        "len(documents), \"documentos indexados en oficios_jina\""
      ],
      "metadata": {
        "id": "ZyUPvGgQA9yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importo AutoTokenizer y AutoModelForCausalLM de la librer√≠a transformers para poder cargar el modelo LLM y su tokenizer desde HuggingFace. El tokenizer convierte texto en tokens y el modelo CausalLM genera texto. Tambi√©n importo torch porque los modelos de lenguaje se ejecutan sobre PyTorch, que maneja los tensores y el dispositivo (CPU/GPU)"
      ],
      "metadata": {
        "id": "_pxUzu9-gQO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esto permite cargar el LLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "A20pvL5GbLX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar el modelo y el tokenizer del LLM\n",
        "\n",
        "Esto:\n",
        "\n",
        "‚úî descarga el modelo\n",
        "\n",
        "‚úî prepara el tokenizer\n",
        "\n",
        "‚úî lo sube a GPU/CPU autom√°ticamente\n",
        "\n",
        "‚úî lo deja listo para generar texto"
      ],
      "metadata": {
        "id": "2lSBK4JvbT-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
        "llm_model  = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "xneVto1obWB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear la clase del chatbot"
      ],
      "metadata": {
        "id": "53uzzRhwbjEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBotRAG:\n",
        "    def __init__(self, collection, embedder, tokenizer, llm):\n",
        "        self.collection = collection\n",
        "        self.embedder = embedder\n",
        "        self.tokenizer = tokenizer\n",
        "        self.llm = llm\n",
        "\n",
        "    def retrieve(self, pregunta, k=3):\n",
        "        # 1. vectorizar la pregunta del usuario\n",
        "        query_emb = self.embedder.encode([pregunta]).tolist()\n",
        "\n",
        "        # 2. buscar en la colecci√≥n los k m√°s similares\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=query_emb,\n",
        "            n_results=k\n",
        "        )\n",
        "\n",
        "        return results[\"documents\"][0]   # devuelve lista de documentos\n",
        "\n",
        "    def generate(self, pregunta, contexto):\n",
        "        # Preparo el prompt estilo instruct\n",
        "        prompt = f\"\"\"Us√° el siguiente contexto para responder de forma clara y corta:\n",
        "\n",
        "Contexto:\n",
        "{contexto}\n",
        "\n",
        "Pregunta del usuario: {pregunta}\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\") #.to(self.llm.device)\n",
        "\n",
        "        output = self.llm.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            #temperature=0.2\n",
        "            do_sample=False,    # respuesta m√°s estable\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        respuesta = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        return respuesta\n",
        "\n",
        "    def answer(self, pregunta):\n",
        "\n",
        "        contexto = self.retrieve(pregunta)\n",
        "\n",
        "        respuesta = self.generate(pregunta, contexto)\n",
        "\n",
        "        return respuesta\n"
      ],
      "metadata": {
        "id": "bx1309o2bhrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear instancias del chatbot usando cada embedding"
      ],
      "metadata": {
        "id": "3fP52Z4JboNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚û°Ô∏è Chatbot con MiniLM"
      ],
      "metadata": {
        "id": "GDLC4LE_bsXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_minilm = ChatBotRAG(\n",
        "    collection=collection_minilm,\n",
        "    embedder=embedding_model_minilm,\n",
        "    tokenizer=tokenizer,\n",
        "    llm=llm_model\n",
        ")\n"
      ],
      "metadata": {
        "id": "econWMfabpZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚û°Ô∏è Chatbot con Jina-es"
      ],
      "metadata": {
        "id": "Nf6mXFMKbtQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_jina = ChatBotRAG(\n",
        "    collection=collection_jina,\n",
        "    embedder=embedding_model_jina,\n",
        "    tokenizer=tokenizer,\n",
        "    llm=llm_model\n",
        ")"
      ],
      "metadata": {
        "id": "WjAQUlrWbuth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probarlos"
      ],
      "metadata": {
        "id": "-X4PP5LJbxYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"¬øC√≥mo me registro como profesional?\"\n",
        "\n",
        "print(\"MiniLM:\")\n",
        "print(chatbot_minilm.answer(pregunta))\n",
        "\n",
        "print(\"Jina:\")\n",
        "print(chatbot_jina.answer(pregunta))"
      ],
      "metadata": {
        "id": "Az3wKIGzbyMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ 1. Funci√≥n para evaluar MiniLM o Jina con tu dataset"
      ],
      "metadata": {
        "id": "A7GItmhgIG3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contexto_contiene_palabras_clave(respuesta_esperada, contexto_str, min_match=3):\n",
        "\n",
        "    #pasamos todo a minuscula\n",
        "    resp_words = set(respuesta_esperada.lower().split())\n",
        "    contexto_words = set(contexto_str.lower().split())\n",
        "\n",
        "    #interseccion-->palabras que estan en ambos lados\n",
        "    matches = resp_words.intersection(contexto_words)\n",
        "\n",
        "    #al menos min_match es decir 3 en comun\n",
        "    return len(matches) >= min_match"
      ],
      "metadata": {
        "id": "Xm2ZDBj1oUex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluar_chatbot(chatbot, eval_set, k=3, nombre_modelo=\"MiniLM\"):\n",
        "    resultados = []\n",
        "    total = len(eval_set)\n",
        "\n",
        "    sum_precision = 0.0\n",
        "    sum_recall = 0.0\n",
        "\n",
        "    for item in eval_set:\n",
        "        pregunta = item[\"pregunta\"]\n",
        "        respuesta_esperada = item[\"respuesta\"]\n",
        "\n",
        "        # Recuperar contexto con k chunks\n",
        "        contexto = chatbot.retrieve(pregunta, k=k)\n",
        "\n",
        "        # Convertir contexto a string para buscar dentro\n",
        "        if isinstance(contexto, list):\n",
        "            contexto_str = \" \".join(contexto)\n",
        "        else:\n",
        "            contexto_str = contexto\n",
        "\n",
        "        # Determinar si el contexto contiene la respuesta esperada\n",
        "        contexto_relevante = contexto_contiene_palabras_clave(respuesta_esperada, contexto_str, min_match=3)\n",
        "\n",
        "        # M√©tricas simples de context precision y recall\n",
        "        if contexto_relevante:\n",
        "            precision_i = 1.0 / k\n",
        "            recall_i = 1.0\n",
        "        else:\n",
        "            precision_i = 0.0\n",
        "            recall_i = 0.0\n",
        "\n",
        "        sum_precision += precision_i\n",
        "        sum_recall += recall_i\n",
        "\n",
        "        # Generar respuesta del chatbot\n",
        "        respuesta_generada = chatbot.generate(pregunta, contexto)\n",
        "\n",
        "        resultados.append({\n",
        "            \"pregunta\": pregunta,\n",
        "            \"respuesta_esperada\": respuesta_esperada,\n",
        "            \"respuesta_generada\": respuesta_generada,\n",
        "            \"contexto\": contexto,\n",
        "            \"contexto_relevante\": contexto_relevante,\n",
        "            \"context_precision_i\": precision_i,\n",
        "            \"context_recall_i\": recall_i\n",
        "        })\n",
        "\n",
        "    # M√©tricas promedio\n",
        "    context_precision_prom = sum_precision / total\n",
        "    context_recall_prom = sum_recall / total\n",
        "\n",
        "    print(f\"\\n=== Resultados {nombre_modelo} (k={k}) ===\")\n",
        "    print(f\"Context precision promedio: {context_precision_prom:.3f}\")\n",
        "    print(f\"Context recall promedio:    {context_recall_prom:.3f}\")\n",
        "\n",
        "    return resultados, context_precision_prom, context_recall_prom\n"
      ],
      "metadata": {
        "id": "8j0PrkjOIGZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ 2. Evaluar MiniLM y Jina"
      ],
      "metadata": {
        "id": "-WXjMVHjKehO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "\n",
        "resultados_minilm, prec_minilm, rec_minilm = evaluar_chatbot(\n",
        "    chatbot_minilm,\n",
        "    qa_soluciones_cliente,\n",
        "    k=k,\n",
        "    nombre_modelo=\"MiniLM\"\n",
        ")\n",
        "\n",
        "resultados_jina, prec_jina, rec_jina = evaluar_chatbot(\n",
        "    chatbot_jina,\n",
        "    qa_soluciones_cliente,\n",
        "    k=k,\n",
        "    nombre_modelo=\"Jina\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "XSqeCa7jKe_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úîÔ∏è Context Precision = 0.286\n",
        "\n",
        "Esto significa que:\n",
        "\n",
        "En promedio, 1 de cada 3.5 chunks recuperados por el RAG fue relevante.\n",
        "\n",
        "Con k=3, el valor ‚Äúperfecto‚Äù ser√≠a 0.333\n",
        "As√≠ que 0.286 es excelente:\n",
        "‚Üí quiere decir que casi siempre, al menos un chunk era √∫til.\n",
        "\n",
        "‚úîÔ∏è Context Recall = 0.857\n",
        "\n",
        "Esto significa que:\n",
        "\n",
        "En el 85.7% de las preguntas del dataset de evaluaci√≥n, el RAG recuper√≥ al menos un fragmento relevante.\n",
        "\n",
        "üü¢ MINILM responde mejor cuando el LLM genera\n",
        "\n",
        "üü£ Jina recupera similarmente pero falla m√°s en respuestas\n",
        "\n",
        "Los resultados fueron id√©nticos para MiniLM y Jina (precision = 0.286, recall = 0.857).\n",
        "\n",
        "Esto se debe a que ambas colecciones contienen los mismos documentos y mi m√©trica eval√∫a literalidad dentro del contexto recuperado. Por lo tanto, ambos modelos identifican fragmentos relevantes con la misma frecuencia.\n",
        "\n",
        "Sin embargo, en la prueba cualitativa observ√© una clara diferencia: MiniLM genera respuestas m√°s completas y naturales, mientras que Jina tiende a ser m√°s r√≠gido y a veces responde que no tiene la informaci√≥n, incluso cuando los chunks recuperados eran adecuados.\n",
        "\n",
        "Por este motivo, aunque las m√©tricas cuantitativas sean iguales, elijo MiniLM como embedding principal para mi chatbot en un entorno real de usuarios."
      ],
      "metadata": {
        "id": "AMLC5tFnzhB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for r in resultados_minilm[:3]:\n",
        "    print(\"PREGUNTA:\", r[\"pregunta\"])\n",
        "    print(\"ESPERADA:\", r[\"respuesta_esperada\"])\n",
        "    print(\"GENERADA:\", r[\"respuesta_generada\"])\n",
        "    print(\"CONTEXT RELEVANTE:\", r[\"contexto_relevante\"])\n",
        "    print(\"-\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "id": "oyNtnjkWKjGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in resultados_jina[:3]:\n",
        "    print(\"PREGUNTA:\", r[\"pregunta\"])\n",
        "    print(\"ESPERADA:\", r[\"respuesta_esperada\"])\n",
        "    print(\"GENERADA:\", r[\"respuesta_generada\"])\n",
        "    print(\"CONTEXT RELEVANTE:\", r[\"contexto_relevante\"])\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "At89aXpoKmu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ê Conclusi√≥n Final del Trabajo\n",
        "\n",
        "En este trabajo implement√© un chatbot basado en arquitectura RAG (Retrieval-Augmented Generation) utilizando mi propio conjunto de conocimientos sobre una aplicaci√≥n de oficios. Para construirlo, arm√© una base de datos de preguntas y respuestas reales de clientes y profesionales, y luego prob√© distintos modelos de embeddings y un modelo LLM para generar respuestas contextualizadas.\n",
        "\n",
        "Primero constru√≠ dos bases vectoriales utilizando MiniLM-L6-v2 y Jina Embeddings v2, ambos modelos de embeddings adecuados para espa√±ol y para preguntas cortas e informales, t√≠picas de usuarios reales. Luego integr√© estas colecciones con el modelo LLM Mistral-7B-Instruct, implementando las funciones de recuperaci√≥n y generaci√≥n dentro de una clase ChatBotRAG.\n",
        "\n",
        "Para evaluar el rendimiento, gener√© un dataset de evaluaci√≥n independiente, con preguntas nuevas que no estaban en la base original. Esto permiti√≥ medir c√≥mo se comportaban los embeddings frente a informaci√≥n que no coincid√≠a literalmente con los textos de la base, simulando dudas reales de usuarios.\n",
        "\n",
        "Utilic√© las m√©tricas Context Precision y Context Recall para medir la calidad de la recuperaci√≥n. Con k=3, ambos modelos lograron un context recall de 0.857, indicando que recuperaron al menos un fragmento relevante en el 86 % de los casos. El context precision (0.286) mostr√≥ que, en promedio, 1 de cada 3 chunks recuperados result√≥ √∫til. Como la m√©trica era estrictamente literal, ambos modelos obtuvieron resultados cuantitativos muy similares, ya que operaban sobre los mismos documentos.\n",
        "\n",
        "Sin embargo, el an√°lisis cualitativo mostr√≥ diferencias importantes:\n",
        "\n",
        "MiniLM gener√≥ respuestas m√°s claras, completas y naturales cuando el LLM combinaba el contexto recuperado con su propio conocimiento.\n",
        "\n",
        "Jina fue m√°s r√≠gido, m√°s sensible a variaciones de contexto y, en ocasiones, respondi√≥ que no pod√≠a contestar incluso con informaci√≥n suficiente recuperada.\n",
        "\n",
        "Esto demuestra que, aunque las m√©tricas autom√°ticas muestren igualdad, la experiencia real del usuario y la calidad de las respuestas generadas dependen de algo m√°s que la recuperaci√≥n literal: dependen del balance entre embeddings, LLM y dise√±o del prompt.\n",
        "\n",
        "En conclusi√≥n, aunque ambos modelos funcionaron correctamente, MiniLM result√≥ el embedding m√°s adecuado para mi caso, ya que permite respuestas m√°s √∫tiles, naturales y consistentes dentro del flujo de la aplicaci√≥n. La integraci√≥n final con Mistral-7B-Instruct logr√≥ un chatbot capaz de contestar preguntas reales usando mi propia base de conocimiento, cumpliendo con el objetivo del trabajo pr√°ctico y dejando preparado un sistema escalable para futuras mejoras y expansi√≥n de datos."
      ],
      "metadata": {
        "id": "ESHzSo3v3UME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "üëâ Link oficial del modelo:\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
        "\n",
        "‚≠êEmbeddings‚≠ê\n",
        "\n",
        "**sentence-transformers/all-MiniLM-L6-v2**\n",
        "\n",
        "üìå Link: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "**jinaai/jina-embeddings-v2-base-es**\n",
        "\n",
        "üìå Link: https://huggingface.co/jinaai/jina-embeddings-v2-base-es\n",
        "\n",
        "ChatGPT (2025). Asistencia conversacional utilizada como apoyo t√©cnico durante el desarrollo del trabajo pr√°ctico. No se adjunta la conversaci√≥n completa por contener elementos personales y no relevantes para la evaluaci√≥n del contenido acad√©mico.\n"
      ],
      "metadata": {
        "id": "8xdn0vOxHbju"
      }
    }
  ]
}